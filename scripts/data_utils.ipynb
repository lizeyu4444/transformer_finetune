{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import random as rd\n",
    "import spacy \n",
    "import pyodbc\n",
    "\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "\n",
    "def read_json(filepath):\n",
    "    with open(filepath, 'r') as fi:\n",
    "        data = json.load(fi)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDAL(object):\n",
    "\n",
    "    def __init__(self, host, user, passwd, name):\n",
    "        driver= '{ODBC Driver 13 for SQL Server}'\n",
    "        self.conn = pyodbc.connect('DRIVER={};SERVER={};DATABASE={};UID={};PWD={}'.format(\n",
    "                               driver, \n",
    "                               host, \n",
    "                               name,\n",
    "                               user,\n",
    "                               passwd))\n",
    "        self.cursor = self.conn.cursor()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.cursor.close()\n",
    "        self.conn.close()\n",
    "\n",
    "    def execute(cls, sql):\n",
    "        cls.cursor.execute(sql)\n",
    "        return cls.cursor.fetchall()\n",
    "\n",
    "    def execute_commit(cls, sql):\n",
    "        cls.cursor.execute(sql)\n",
    "        cls.conn.commit()\n",
    "        \n",
    "db = BaseDAL(host='10.173.1.150', user='sa', passwd='wandagcLi4$', name='zx-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 规则匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegexMatch(object):\n",
    "    def __init__(self, regex_path):\n",
    "        if not os.path.exists(regex_path):\n",
    "            raise ValueError('Must provide regex file path.')\n",
    "        self.neg_rules, self.neg_exact = [], []   # 消极\n",
    "        self.pos_rules, self.pos_exact = [], []   # 积极\n",
    "        self.norm_rules, self.norm_exact = [], [] # 正常\n",
    "        with open(regex_path, 'r') as fi:\n",
    "            lines = fi.readlines()\n",
    "            lines = [line.strip() for line in lines if line.strip()]\n",
    "        for line in lines:\n",
    "            if line.startswith('neg'):\n",
    "                line = line.split('\\t')\n",
    "                if line[1].strip() == 'regex':\n",
    "                    self.neg_rules.append(re.compile(line[2].strip()))\n",
    "                elif line[1].strip() == 'exact':\n",
    "                    self.neg_exact.append(line[2].strip())\n",
    "            elif line.startswith('pos'):\n",
    "                line = line.split('\\t')\n",
    "                if line[1].strip() == 'regex':\n",
    "                    self.pos_rules.append(re.compile(line[2].strip()))\n",
    "                elif line[1].strip() == 'exact':\n",
    "                    self.pos_exact.append(line[2].strip())\n",
    "            elif line.startswith('norm'):\n",
    "                line = line.split('\\t')\n",
    "                if line[1].strip() == 'regex':\n",
    "                    self.norm_rules.append(re.compile(line[2].strip()))\n",
    "                elif line[1].strip() == 'exact':\n",
    "                    self.norm_exact.append(line[2].strip())\n",
    "            else:\n",
    "                print('Warning: rule format is not accepted -->', line[:30])\n",
    "\n",
    "    def process(self, sentence):\n",
    "        res = {}\n",
    "        neg_matched, pos_matched = [], []\n",
    "        neg_texts, pos_texts = [], []\n",
    "        for rule in self.neg_rules:\n",
    "            r = re.search(rule, sentence)\n",
    "            if r:\n",
    "                neg_matched.append(rule)\n",
    "                neg_texts.append(r.group())\n",
    "        for rule in self.pos_rules:\n",
    "            r = re.search(rule, sentence)\n",
    "            if r:\n",
    "                pos_matched.append(rule)\n",
    "                pos_texts.append(r.group())\n",
    "        res['neg_rules'] = neg_matched\n",
    "        res['neg_texts'] = neg_texts\n",
    "        res['pos_rules'] = pos_matched\n",
    "        res['pos_texts'] = pos_texts\n",
    "        return res\n",
    "    \n",
    "    def run(self, sentences):\n",
    "        results = []\n",
    "        for sent in sentences:\n",
    "            res = self.process(sent)\n",
    "            results.append(res)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'neg_rules': [re.compile(r'(业绩|利润|盈利|营收|净利).{0,4}(大跌|下滑|降|减少|双降|预减|亏损)',\n",
       "   re.UNICODE)],\n",
       "  'neg_texts': ['营收、净利双下滑'],\n",
       "  'pos_rules': [],\n",
       "  'pos_texts': []},\n",
       " {'neg_rules': [re.compile(r'(业绩|利润|盈利|营收|净利).{0,4}(大跌|下滑|降|减少|双降|预减|亏损)',\n",
       "   re.UNICODE)],\n",
       "  'neg_texts': ['净利润预减'],\n",
       "  'pos_rules': [],\n",
       "  'pos_texts': []},\n",
       " {'neg_rules': [re.compile(r'(业绩|利润|盈利|营收|净利).{0,4}(大跌|下滑|降|减少|双降|预减|亏损)',\n",
       "   re.UNICODE)],\n",
       "  'neg_texts': ['营收净利润双下滑'],\n",
       "  'pos_rules': [],\n",
       "  'pos_texts': []},\n",
       " {'neg_rules': [re.compile(r'(被|遭|股份|万股).{0,4}(冻结|罚款|重罚)', re.UNICODE)],\n",
       "  'neg_texts': ['股份已被冻结'],\n",
       "  'pos_rules': [],\n",
       "  'pos_texts': []}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['房企业绩观察 | 土储成发展短板 冠城大通营收、净利双下滑遭问询', \n",
    "        '上半年净利润预减超三成 重药控股启动并购准备抢“地盘”',\n",
    "        '跨界转型受阻 浙江广厦营收净利润双下滑遭问询', \n",
    "        '浙江广厦(600052.SH)：楼忠福及楼明合计3.5%股份已被冻结']\n",
    "\n",
    "rm_path = '../regex/sentment_rule.txt'\n",
    "rm = RegexMatch(rm_path)\n",
    "rm.run(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = db.execute(\n",
    "    '''select file_uuid,title\n",
    "    from [zx-test].[dbo].[news]\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db(d):\n",
    "    file_uuid, sentence = d[0], d[1]\n",
    "    res = rm.process(sentence)\n",
    "    neg_texts, pos_texts = [], []\n",
    "    if res['neg_texts']:\n",
    "        neg_texts = res['neg_texts']\n",
    "    if res['pos_texts']:\n",
    "        pos_texts = res['pos_texts']\n",
    "    if neg_texts or pos_texts:\n",
    "        neg_texts = ','.join(neg_texts)\n",
    "        pos_texts = ','.join(pos_texts)\n",
    "        db.execute_commit(\n",
    "            '''update [zx-test].[dbo].[news]\n",
    "            set neg_texts = N'{}', pos_texts = N'{}' \n",
    "            where file_uuid = '{}'\n",
    "            '''.format(neg_texts, pos_texts, file_uuid)\n",
    "        )\n",
    "_ = list(map(lambda d: update_db(d), data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 匹配负面词汇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BowMatch(object):\n",
    "    def __init__(self):\n",
    "        self.bow_path = '../data/中文金融情感词典_姜富伟等(2020).xlsx'\n",
    "        self.init()\n",
    "        \n",
    "    def init(self):\n",
    "        neg_bow = pd.read_excel(self.bow_path, sheet_name='negative')\n",
    "        pos_bow = pd.read_excel(self.bow_path, sheet_name='positive')\n",
    "        neg_bow = map(lambda x: x.strip(), neg_bow.values.flatten())\n",
    "        pos_bow = map(lambda x: x.strip(), pos_bow.values.flatten())\n",
    "        self.neg_bow = set([i for i in neg_bow if i])\n",
    "        self.pos_bow = set([i for i in pos_bow if i])\n",
    "        \n",
    "    def process(self, word):\n",
    "        if word in self.neg_bow:\n",
    "            return 2\n",
    "        elif word in self.pos_bow:\n",
    "            return 1\n",
    "        return 0\n",
    "    \n",
    "    def process_sentence(self, sent):\n",
    "        neg_words, pos_words = [], []\n",
    "        for word in jieba.cut(sent):\n",
    "            m = self.process(word)\n",
    "            if m == 2:\n",
    "                neg_words.append(word)\n",
    "            elif m == 1:\n",
    "                pos_words.append(word)\n",
    "        return neg_words, pos_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(filename):\n",
    "    filepath = '../data/raw/{}.json'.format(filename)\n",
    "    data = read_json(filepath)\n",
    "    bm = BowMatch()\n",
    "    for i in range(int(len(data)/1000)+1):\n",
    "        new_file = '../data/标注/{}_{}.xlsx'.format(filename, i)\n",
    "        res = []\n",
    "        left, right = i*1000, min((i+1)*1000, len(data))\n",
    "        for row in data[left:right]:\n",
    "            row_ = [row['symbol'], row['publish_date'], row['title'], None, None, None, row['content'], row['url']]\n",
    "            neg, pos = bm.process_sentence(row['title'])\n",
    "            row_[3] = neg\n",
    "            row_[4] = pos\n",
    "            res.append(row_)\n",
    "        df = pd.DataFrame(res, columns=['symbol', 'publish_date', 'title', '负向词', '正向词', '规则', 'content', 'url'])\n",
    "        df.to_excel(new_file, index=False)\n",
    "\n",
    "process('xueqiu')\n",
    "process('sina')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 转化标注的文件为训练格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6215, 2)\n"
     ]
    }
   ],
   "source": [
    "id_to_label = {\n",
    "    '1': '负面',\n",
    "    '2': '中性',\n",
    "    '3': '正面'\n",
    "}\n",
    "\n",
    "def read_labeled_files():\n",
    "    labeled_files = glob('../data/标注_0819/sina_*.xlsx')\n",
    "    data = []\n",
    "    for file in labeled_files:\n",
    "        df = pd.read_excel(file)\n",
    "        rows = df.iloc[:, [2, 5]].values.tolist()\n",
    "        data.extend(rows)\n",
    "    df = pd.DataFrame(data, columns=['sentence', 'label'])\n",
    "    return df\n",
    "def process_labeled_data(df):\n",
    "    df_ = df.copy()\n",
    "    df_['sentence'] = df_['sentence'].map(lambda x: x.replace('\\t', ''))\n",
    "    df_['label'] = df_['label'].map(lambda x: id_to_label[str(x)])\n",
    "    is_q = df_['sentence'].map(lambda x: x.startswith('投资者提问'))\n",
    "    df_ = df_[-is_q]\n",
    "    print(df_.shape)\n",
    "    return df_\n",
    "df = read_labeled_files()\n",
    "df = process_labeled_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>北向资金高比例持有索菲亚、上海机场等股</td>\n",
       "      <td>正面</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>上海机场：短期业绩承压明显 复苏时间尚难确定</td>\n",
       "      <td>负面</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>深交所连预警：3股将被外资买爆 为何钟爱中国资产？</td>\n",
       "      <td>正面</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>上海机场受沪股通青睐 连续3日净买入</td>\n",
       "      <td>正面</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>首创股份拟发行20亿元可续期公司债券</td>\n",
       "      <td>中性</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sentence label\n",
       "0        北向资金高比例持有索菲亚、上海机场等股    正面\n",
       "3     上海机场：短期业绩承压明显 复苏时间尚难确定    负面\n",
       "4  深交所连预警：3股将被外资买爆 为何钟爱中国资产？    正面\n",
       "5         上海机场受沪股通青睐 连续3日净买入    正面\n",
       "6         首创股份拟发行20亿元可续期公司债券    中性"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "np.random.seed(42)\n",
    "\n",
    "shuffle_index = np.random.permutation(len(df))\n",
    "train_data = df.iloc[shuffle_index[:-int(len(df)*test_ratio)]]\n",
    "test_data = df.iloc[shuffle_index[-int(len(df)*test_ratio):]]\n",
    "\n",
    "train_data.to_csv('../data/processed/train.tsv', sep ='\\t', index=False)\n",
    "test_data.to_csv('../data/processed/dev.tsv', sep ='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_excel('../data/raw/嘉华科技.xlsx')\n",
    "\n",
    "## 存储为模型预测的格式\n",
    "# df = df[['title']]\n",
    "# df.columns = ['sentence']\n",
    "# df.to_csv('../data/processed/test.tsv', sep ='\\t', index=False)\n",
    "\n",
    "## 合并\n",
    "# df_pred = pd.read_excel('../data/prediction/st_news_test_results.xlsx')\n",
    "# df['pred'] = df_pred['pred_label']\n",
    "# df['probability'] = df_pred['probability']\n",
    "# df.to_excel('../data/trash/嘉华科技_pred.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. NER公司名提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('zh_core_web_sm')\n",
    "\n",
    "def ner_detect(sentence):\n",
    "    s = nlp(sentence)\n",
    "    return [i.text for i in s.ents if i.label_ == 'ORG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['宝钢股份(', '宝山基地']\n"
     ]
    }
   ],
   "source": [
    "sent = '浦发银行哈尔滨分行：以金融力量助力企业复工复产'\n",
    "sent = '华能水电：公司电价的定价收费优先电厂电价执行政府定价 市场化电厂电价由市场化交易形成'\n",
    "sent = '宝钢股份(600019.SH)：宝山基地四号高炉设备故障 未造成人员伤亡'\n",
    "res = ner_detect(sent)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "宝山基地\n"
     ]
    }
   ],
   "source": [
    "def ner_match(ner, sentence):\n",
    "    res = re.search(ner, sentence)\n",
    "    if res:\n",
    "        return res.group()\n",
    "\n",
    "ner = '宝山基地'\n",
    "sent = '宝钢股份(600019.SH)：宝山基地四号高炉设备故障 未造成人员伤亡'\n",
    "res = ner_match(ner, sent)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 生成词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_jieba(stopwords_file=None, idf_file=None):\n",
    "    if stopwords_file:\n",
    "        jieba.analyse.set_stop_words(stopwords_file)\n",
    "    if idf_file:\n",
    "        jieba.analyse.set_idf_path(idf_file)\n",
    "init_jieba('../data/stopwords.txt')\n",
    "\n",
    "def build_fn(symbol_pattern, offset=0):\n",
    "    def generate_cloud_words(x):\n",
    "        # x: pd.Series\n",
    "        res = jieba.analyse.extract_tags(x['title'], withWeight=True, topK=300)\n",
    "        res = [i for i in res if not re.search('[a-zA-Z0-9]', i[0])] #re.search('^\\d*\\.?\\d*$', i[0])\n",
    "        m = re.findall(symbol_pattern, x['symbol'])\n",
    "        if len(m) > 0:\n",
    "            m = m[0][offset:]\n",
    "            with open('../data/cloud_words/{}.txt'.format(m), 'w') as fi:\n",
    "                for i in res:\n",
    "                    fi.write('{},{}\\n'.format(i[0], str(i[1])))\n",
    "    return generate_cloud_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read from local files\n",
    "filepath = '../data/raw/{}.json'.format('sina')\n",
    "data = read_json(filepath)\n",
    "df = pd.DataFrame(data)\n",
    "df = pd.DataFrame(\n",
    "    df[['title', 'symbol']].groupby('symbol')['title'].apply(lambda x: ' '.join(x.values))\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_cloud_words = build_fn('<span>\\(\\d+', 7)\n",
    "_ = df.apply(generate_cloud_words, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Read from sql server\n",
    "data = db.execute(\n",
    "    '''select title,symbol,stock_name\n",
    "    from [zx-test].[dbo].[news]\n",
    "    where symbol = 'SH688051'\n",
    "    '''\n",
    ")\n",
    "data = pd.Series({\n",
    "    'symbol': data[0][1],\n",
    "    'title': ' '.join([i[0] for i in data])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_cloud_words = build_fn('[SH|SZ](\\d+)')\n",
    "generate_cloud_words(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 将模型及正则匹配结果保存至数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "ckpt_path = '/tmp/st_news'\n",
    "config = AutoConfig.from_pretrained(ckpt_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n",
    "model_ft = AutoModelForSequenceClassification.from_pretrained(ckpt_path, config=config)\n",
    "model_ft.eval()\n",
    "\n",
    "def predict(sentence):\n",
    "    input_ids = tokenizer.encode(sentence)\n",
    "    input_ids = torch.tensor([input_ids])\n",
    "    with torch.no_grad():\n",
    "        outputs = model_ft(input_ids)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    predictions = softmax(predictions)\n",
    "    index_pred = torch.argmax(predictions[0, :]).item()\n",
    "    prob = predictions.numpy()[0, index_pred]\n",
    "    return index_pred, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = db.execute(\n",
    "    '''select file_uuid,title\n",
    "    from [zx-test].[dbo].[news]\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db(d):\n",
    "    file_uuid, sentence = d[0], d[1]\n",
    "    index_pred, prob = predict(sentence)\n",
    "\n",
    "    db.execute_commit(\n",
    "        '''update [zx-test].[dbo].[news]\n",
    "        set label = '{}', score = '{:.5f}' \n",
    "        where file_uuid = '{}'\n",
    "        '''.format((index_pred+1), prob, file_uuid)\n",
    "    )\n",
    "_ = list(map(lambda d: update_db(d), data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
